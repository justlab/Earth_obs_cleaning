---
title: "Reducing retrieval error in MAIAC AOD with the `Earth_obs_cleaning` machine-learning workflow: Results from CONUS"

bibliography: references.json
csl: https://raw.githubusercontent.com/citation-style-language/styles/795ad0c77258cb7e01f3413123b5b556b4cb6a98/dependent/journal-of-geophysical-research-atmospheres.csl

format:
  html:
    embed-resources: true
  docx: {}

echo: FALSE
---

```{r basics}
source.files = "globals libraries util data modeling paper_functions"
for (filename in strsplit(source.files, " ")[[1]])
     source(sprintf("code/%s.R", filename))
suppressPackageStartupMessages(
   {library(patchwork)
    library(gridExtra)
    library(flextable)
  })
the.sat = tools::toTitleCase(Wf$satellite)
IG = knitr::include_graphics
```

Rendered: `r Sys.time()`

# Abstract

We present a modular and reproducible Open Source software tool that ingests remotely sensed data for supervised machine learning (ML) of the difference between Earth observation (EO) retrievals and ground truth observations in order to generate bias-corrected products.

# Short summary

We demonstrate a reproducible workflow to reduce retrieval error from satellite-based datasets with an application to MODIS Aerosol Optical Depth over the conterminous United States 2000-2022. It facilitates the often laborious task of obtaining and comparing ground-based and remotely sensed quantities for product validation and evaluating retrieval biases. The ML data-driven approach demonstrates impressive improvements in reducing retrieval error with real-world impacts for air quality modeling.

# Introduction

Many types of EO are validated using "gold standard" datasets obtained from networks of ground stations.[1--4](https://paperpile.com/c/ze3giJ/fWox+jvWM+EDSi+RF7S) When satellite observations are aligned with these ground measures in space and time, and the ground station data are considered top quality and are observations of the same phenomenon, then we can use the difference in these quantities as an operational measure of retrieval error (setting aside additional uncertainty in the measurements). Making these EO-to-ground comparisons is a key task in product validation and is often conducted globally and for particular space-time regions or within the conditions of particular co-occurring phenomena. While various metrics for these validation comparisons include an assessment of mean bias and visualizations of mean differences,[5](https://paperpile.com/c/ze3giJ/vA4s) an overall summary of these differences across a collocated dataset is the Mean Square Error (MSE). The process of adjusting an EO dataset to account for systematic error, conditional on covariates, is also known as bias-correction and can lead to datasets that more accurately reflect the ground truth. Using data from the ground-stations to define our desired output, along with covariates from the same EO dataset that explain the retrieval error, makes this process "data-driven". The application of a leading supervised machine-learning approach, extreme gradient boosting (XGBoost),[6](https://paperpile.com/c/ze3giJ/72SB) which can capture complex interactions and outperforms related algorithms,[@just2018] has great empirical potential so long as care is taken to avoid overfitting (e.g., capturing phenomena that would not generalize to new data).[7--9](https://paperpile.com/c/ze3giJ/NejN+4wlV+gFoW) 

We have previously demonstrated a closely-related data-driven analysis in the Northeast USA but lacked reproducible analytic code or a configurable and automated software tool.[7](https://paperpile.com/c/ze3giJ/NejN) Our XGBoost model empirically corrects biases related to complex interactions of acquisition/sensor and regional characteristics (e.g., aerosol composition or surface brightness) without reliance on any external data assimilation. Using an interpretability method, such as Shapley Additive Explanations (SHAP),[11](https://paperpile.com/c/ze3giJ/TgOX) to interpret our predictions, we see that our model accounts for complex interactions with a physical basis (e.g., challenging backscattering configurations). We also adapted our approach to refine MAIAC Column Water Vapor (CWV) achieving a 27% decrease in Terra CWV RMSE, with external validation at independent SuomiNet GPS receivers.[8](https://paperpile.com/c/ze3giJ/4wlV)

Satellite-borne EO retrievals are physics-based and often validated with reliable ground networks. Building collocated datasets of EO over regions and time periods of interest is facilitated by both specialized tools for particular products (e.g., those from LP DAAC[14](https://paperpile.com/c/ze3giJ/KNHa) and ORNL DAAC[15](https://paperpile.com/c/ze3giJ/lQFH), although neither support MAIAC AOD) and structured access protocols such as OPeNDAP.[16](https://paperpile.com/c/ze3giJ/4m2u) When validating aerosol retrievals with ground-based observations, it is a common convention to spatially average retrieval products \-- even up to a 40 km \* 40 km area.[3](https://paperpile.com/c/ze3giJ/EDSi) This appears, at least in part, to be a legacy of 1) desiring to calculate local variance in products that were coarser (10 km resolution) than the more recent retrieval products like 1 km MAIAC and 2) a concern that accommodating temporal mismatches that were larger under prior AERONET sampling regimen (our analysis of AERONET 2018-2021 over CONUS finds an overall station-level median retrieval frequency of 5 minutes) in turn requires spatial averaging (since an aerosol air mass may move at 50 km/h or \~13.9 m/s).[17](https://paperpile.com/c/ze3giJ/TRsT) The consequence of these large-area averaged comparisons is that they do not directly apply to the performance of the product at the original resolution (e.g., 1 km for MAIAC). Leveraging data-driven tools to validate high resolution (e.g., 1 km) products enables a time-efficient and cost-effective ML-based technology for historical and near real-time validation and bias correction. Thus a configurable tool also offers finer spatio-temporal domains (versus commonly global validation papers) with single pixel performance. 

The GMAO MERRA-2[18](https://paperpile.com/c/ze3giJ/QSDA) uses a data-driven neural network retrieval (NNR) for AOD directly training on cloud-screened MODIS (as well as other sensors) observed radiances along with key modifiers of light scattering and retrieval error (e.g, view geometry, cloud fraction, and surface albedo) versus AERONET AOD.[19](https://paperpile.com/c/ze3giJ/1aAj) The NNR relies on the older MODIS AOD Collection 5 (C5) (while currently available is Collection 6.1 (C6.1)[20](https://paperpile.com/c/ze3giJ/oGky)), and does not build on the physics-based retrievals of MODIS operational products. The NNR lacks interpretability in how it differs from physics-based retrievals, and ultimately offers no code or reproducible tool/approach that can be adapted for other remote sensing products. A key distinction of our approach is that we model the difference between the retrieval and the ground observation as our target to be estimated, rather than predicting the retrieval product (e.g., AOD) directly. Our data-driven bias correction strategy leverages and supports the physics-based retrieval products while making the key predictors those variables that are informative on retrieval error rather than the first-principles target phenomenon. 

ORNL's Fixed Sites Subsets Tool[21](https://paperpile.com/c/ze3giJ/ScfT) generates collocated land product datasets; it does not include MAIAC AOD. NASA also has several sophisticated graphical tools and powerful APIs ranging from Worldview to EarthData search that serve as visualization and data access platforms, however these generally do not handle collocation with ground measures or any retrieval-error correction. Several groups, including our own, have published on retrieval error/bias correction on Low Earth orbit (for MODIS[7,22,23](https://paperpile.com/c/ze3giJ/01P2+NejN+9sah)) and Geostationary satellite-based datasets (ABI[2](https://paperpile.com/c/ze3giJ/jvWM)) demonstrating data-driven approaches in the research stage, although our publication with CWV[8](https://paperpile.com/c/ze3giJ/4wlV) is the only one with public code sharing.[24](https://paperpile.com/c/ze3giJ/1rUv) To our knowledge, we are presenting the first tool that will enable collocation and improved output data in a reproducible and performant workflow. Although demonstrated for CONUS, we have developed flexible user parameterization for a reproducible workflow tool that can be deployed on any Linux system.

# Methods

### Study Domain

The study region was the conterminous United States (CONUS) and all match-ups of MODIS data were based on the 1 km grid cells in the Global sinusoidal Coordinate Reference System. The study period included all retrievals from `r min(Wf$dates)` through `r max(Wf$dates)`. The `Earth_obs_cleaning` workflow automatically downloaded all intersecting tiles of MCD19A2 Collection 6.1 [@lyapustinalexei2018] based on a NASA-supplied MODIS tile boundary file. While the MCD19A2 product includes data from both Aqua and Terra MODIS instruments, we focus our analyses on the longer running Terra.

### AERONET Data

The workflow retrieved the global AERONET archive (Version 3 Direct Sun algorithm) that was last modified `r lubridate::as_date(tz = "UTC", file.mtime(tar_read(aer_orig_obs_path)))`. Only Level 2.0 (cloud screened and quality-assured) data were used. AERONET data were interpolated to 470 nm, matching the MAIAC blue-band AOD retrieval, using a prediction from a linear regression for each AERONET site-time with a quadratic term on log-transformed observations using AERONET's reported 'exact wavelengths', restricted to a subset of relevant wavelengths (≤1000 nm) where there was at least one observation less than and greater than 470 nm.

MAIAC AOD retrievals from all overpasses in the overlying 1 km grid cell were matched to the closest AERONET observation in time (no more than 8 minutes). Because the purpose of the `Earth_obs_cleaning` workflow is to reduce retrieval error, there was no restriction based on quality codes of input MAIAC data. The targeted estimand in all analyses was the difference of the two AODs (*diff~AOD~*): MAIAC AOD minus AERONET observation, our empirical estimate of retrieval error.

### Predictors

The model to predict *diff~AOD~* included the following 12 predictors which were all extracted from the Science Dataset (SDS) of the MCD19A2 product or calculated over spatial focal windows (within tile-overpass):

-   the MAIAC AOD

-   the time of the satellite acquisition

-   the blue-band AOD uncertainty

-   cosine of the Solar Zenith Angle

-   cosine of the View Zenith Angle

-   Relative Azimuth

-   Scattering Angle

-   Glint Angle

-   Column Water Vapor

-   a dichotomous indicator for QA_best

-   the mean AOD calculated from a square window with an 11 km side-length

-   the proportion missing AOD from a square window with an 11 km side-length

Thus, the `Earth_obs_cleaning` workflow uses only SDS layers and predictors derived from the original EO product and includes no external meteorology or data assimilation.

### Modeling and performance metrics

Modeling used XGBoost for regression with a squared error objective function and the Dropouts meet Multiple Additive Regression Trees (DART) booster. Briefly, 50 candidate sets of hyperparameters, which govern the flexibility of the gradient boosting model, were generated using an efficient latin hypercube sampling and evaluated within nested cross-validation (internal to the training data) and with several limits on overall model complexity for parsimony, speed, and to avoid overfitting. Because data splitting used a 5-fold site-level block cross validation, we thus only evaluated the corrected AOD at sites that were never used in model training.

Our primary metric to evaluate the difference between MAIAC and AERONET before and after applying our correction was the root mean square error (RMSE), which is at the original scale of the AOD and can be compared in magnitude with the overall variability based on the standard deviation (SD) of the AERONET AOD. We also report the SD for raw and corrected MAIAC AOD to examine how our empirical adjustments impact variability. We calculate bias (the mean difference of satellite AOD versus AERONET observations) for the raw and corrected products. We also report the Pearson correlation of the raw and corrected products versus the collocated AERONET measures. We plotted the agreement of raw and corrected AOD versus AERONET AOD in scatterplots for the interval [0, 1] given that almost all AERONET observations in the cross-validation set fell within that range. To generate empirical error envelopes with performance for both low and high AOD conditions, we optimized the combination of additive and multiplicative terms closest to containing \~2/3 of observations for AERONET AOD below and above 0.6, a threshold previously used when evaluating MAIAC AOD.

While the model was trained using all of the collocated observations generated from within the study domain defined above, we present a number of stratified tables with the same metrics listed above to examine performance of the correction procedure in various data subsets. Stratification variables included time/space (NOAA climate regions, months of the year, and by year), atmospheric conditions (low versus high AOD), and two additional factors related to MAIAC retrieval (MAIAC quality flag and number of overpasses per day).

### Model interpretation with SHAP

We used SHAP, generated for the cross-validated predictions, to interpret the contribution of each predictor to the predictions. Our summary metric of variable importance was the mean of the absolute SHAP for each predictor, and individual SHAPs were plotted as a time series and versus predictor values for exploratory visualizations.

### New predictions and comparison with ground monitors

After cross validation, the model was refit with the entire training data set and predictions were generated to correct AOD in new locations (i.e., grid cells without AERONET sites) on demand. The `Earth_obs_cleaning` workflow stores these in Parquet format for efficient data storage and retrieval. To demonstrate that corrected AOD still shows spatial patterns consistent with expectations we generated maps of raw and corrected MAIAC on a typical day (selected as the tile with the most non-missing AOD on the day with the median improvement in MSE for AERONET stations). To show the correction in known high-AOD conditions we plotted the Baltimore, MD area on June 10, 2015 which had substantial subsiding long-range transported Canadian wildfire smoke as previously documented with extensive surface and atmospheric monitoring [@dreessen2016].

Beyond cross-validation performance, we also sought to evaluate the utility of applying the `Earth_obs_cleaning` workflow for the correlation of satellite AOD with surface measurements of PM~2.5~ from the broad network of air quality monitors in the Environmental Protection Agency's Air Quality System (AQS). We downloaded all 24-hour measures of PM~2.5~ concentration (using the Federal Reference Method or Federally Equivalent Method; Parameter Code 88101) from all AQS sites within CONUS. We joined each unique monitor with AOD for the overlying grid cell for each overpass occurring in the same day. We then reweighted by the inverse of the number of PM~2.5~\~AOD matchups in each cell-day and calculated the Pearson correlation using either MAIAC AOD or corrected AOD.

# Results

## Main findings from cross-validation

```{r get_cv}
dt <- copy(tar_read(cv)$mDT_wPred)
```

After collocation there were `r scales::comma(nrow(dt))` matched AOD retrievals from `r uniqueN(dt$site)` AERONET sites in CONUS. The quartiles of the absolute difference from the labeled overpass time (one per 1200 km wide tile) and the time of the matched AERONET observation were `r tqs = round(quantile(dt[, abs(as.numeric(difftime(units = "secs", time.sat, time.ground)))], c(.25, .5, .75))); tqs["25%"]` s, `r tqs["50%"]` s, and `r tqs["75%"]` s. While the distribution of AOD is skewed, our estimate of retrieval error and the dependent variable for our predictive model, *diff~AOD~*, was fairly symmetric and Gaussian (see @fig-dv-density). 

```{r plot_dv}
#| label: fig-dv-density
#| fig-cap: !expr caption
caption = with(list(d = dt), sprintf(
    "Histogram of the difference in collocated AOD (n=%s). Not shown: %s %s",
    scales::comma(d[, .N]),
    d[y.diff < -1 | y.diff > 1, .N],
    "AOD differences outside [-1, 1]"))
p1 <- ggplot(dt, aes(x = y.diff)) +
  geom_histogram(binwidth = 0.005) +
  coord_cartesian(xlim = c(-1, 1), expand = FALSE) +
  xlab("MAIAC AOD minus AERONET AOD") +
  theme_classic()
t1 <- summary(dt[, list(y.diff)], digits = 2)
colnames(t1) <- ""
t1 <- tableGrob(t1, theme=ttheme_minimal(), rows=NULL)  # transform into a tableGrob
p1 + inset_element(t1, left = 0.02, bottom = 0.6, right = 0.25, top = .98)
```

@tbl-cv-overall includes the performance metrics for the full cross-validated training data set. `r sdn = \(x) sqrt(mean((x - mean(x))^2)); f = \(x) sprintf("(SD %.03f)", sdn(dt[[x]])); "";` Here **raw** is Collection 6.1 MAIAC AOD 470nm `r f("y.sat")`; **ground** is AERONET AOD interpolated to 470nm `r f("y.ground")`; and **corrected** is MAIAC after applying an XGBoost-modeled correction term `r f("y.ground.pred")`. The ground values are correlated `r dt[, sprintf("%+.03f", cor(y.ground, y.sat))]` with the raw values and `r dt[, sprintf("%+.03f", cor(y.ground, y.ground.pred))]` with the corrected values.

```{r}
#| label: tbl-cv-overall
#| tbl-cap: Overall performance from cross-validation.
flextable(pretty.table.numbers(tar_read(cv.summary)[1, -"Year"]))
```

## Stratified results for cross validation dataset

Here we show our performance metrics in different subsets of the cross-validation results. These are all from the same model training, just slicing up how the performance looks in different contexts.

```{r noaa_setup}
noaa.regions = list(
  # https://www.ncdc.noaa.gov/monitoring-references/maps/us-climate-regions.php
    Ohio.Valley = c("IL", "IN", "KY", "MO", "OH", "TN", "WV"), #previously 'Central'
    Upper.Midwest = c("IA", "MI", "MN", "WI"), #previously 'East.North.Central'
    Northeast = c("DC", "CT", "DE", "ME", "MD", "MA", "NH", "NJ", "NY", "PA", "RI", "VT"),
      # Washington, DC, has no assigned NOAA climate region, so treat
      # it as part of Maryland.
    Northwest = c("ID", "OR", "WA"),
    South = c("AR", "KS", "LA", "MS", "OK", "TX"),
    Southeast = c("AL", "FL", "GA", "NC", "SC", "VA"),
    Southwest = c("AZ", "CO", "NM", "UT"),
    West = c("CA", "NV"),
    Northern.Rockies.and.Plains = c("MT", "NE", "ND", "SD", "WY")) #previously 'West.North.Central'

state.abb.plusdc = c(state.abb, "DC")
conus.states.plusdc = sort(setdiff(state.abb.plusdc, c("AK", "HI")))

stopifnot(identical(
    sort(unlist(noaa.regions, use.names = F)),
    conus.states.plusdc))

state.noaa.region = factor(
   sapply(state.abb.plusdc, USE.NAMES = F,
       function(sa) if (sa %in% c("AK", "HI")) NA else
           names(noaa.regions)[which(sapply(noaa.regions, function(r)
               sa %in% r))]),
   levels = names(noaa.regions))

# Contiguous US state boundaries
conus = st_transform(crs = crs.us.atlas, get_conus())
```

```{r get_noaa_regions, include=FALSE}
# assign AERONET sites to states
tar_load(aer_stations)
aer_stations_sf <- st_transform(crs = st_crs(conus), st_as_sf(aer_stations, coords = c("lon", "lat"), crs = "epsg:4326"))
aer_stations_sf <- st_join(aer_stations_sf, conus)
colnames(aer_stations_sf)[colnames(aer_stations_sf) == "Site_Name"] = "site"
dt <- merge(dt, aer_stations_sf[, c("site", "STUSPS")], by = "site")
# some sites don't fall within land polygons for whatever reason
dt[is.na(STUSPS), table(site)]
# where are the sites that didn't fall within a state?
site_nostate <- st_as_sf(dt[is.na(STUSPS), ][unique(site), .(site, geometry), mult = "first"])
# ggplot() +
#   geom_sf(data = conus, fill = NA) +
#   geom_sf(data = site_nostate, color = "red")
# what proportion of data are not in a state?
dt[, proportions(table(is.na(STUSPS)))]
# assign these to the closest state
matched <- st_join(site_nostate,
                   conus,
                   join=nngeo::st_nn, k=1)
# plot these matchups
# ggplot() +
#   geom_sf(data = conus, fill = NA) +
#   geom_sf_text(data = matched, aes(label = STUSPS), color = "red") + theme_minimal()
dt[as.data.table(matched, key = "site"), STUSPS := i.STUSPS]

# where do our data come from (by State)?
dt[, .(uniqueN(site), .N), by = STUSPS][order(N, decreasing = T)]

# assign NOAA region from state USPS codes
dt[, state.noaa.region := factor(
   sapply(STUSPS, USE.NAMES = F,
       function(sa) if (sa %in% c("AK", "HI")) NA else
           names(noaa.regions)[which(sapply(noaa.regions, function(r)
               sa %in% r))]),
   levels = names(noaa.regions))]
```

```{r}
#| label: tbl-cv-noaa-regions
#| tbl-cap: Performance by NOAA climate regions. Sorted by decreasing relative improvement in MSE after correction.
by.region = (dt
    [, eval(performance.j), by = state.noaa.region]
    [order(`Proportion of raw MSE`)])
flextable(pretty.table.numbers(by.region))
```

```{r noaa_region_plot, dev='png', dpi=300}
#| label: fig-cv-noaa-regions
#| fig-cap: Map of performance (RMSE) by NOAA regions.
conus$state.noaa.region <- factor(
   sapply(conus$STUSPS, USE.NAMES = F,
       function(sa) if (sa %in% c("AK", "HI")) NA else
           names(noaa.regions)[which(sapply(noaa.regions, function(r)
               sa %in% r))]),
   levels = names(noaa.regions))
conus <- merge(conus, by.region, by = "state.noaa.region")
conus_melt <- melt(as.data.table(conus[, c("geometry", "state.noaa.region", "RMSE, raw", "RMSE, corrected")]),
                   id.vars = c("geometry", "state.noaa.region"))
levels(conus_melt$variable) <- c("MAIAC AOD", "Corrected AOD")
map_region_rmse <- ggplot(conus_melt, aes(geometry = geometry)) +
  geom_sf(aes(fill = value)) +
  facet_wrap(~variable, ncol = 1) +
  coord_sf(default_crs = st_crs(conus)) +
  scale_fill_continuous("Regional\nRMSE", trans = 'reverse') +
  theme_minimal() +
  theme(axis.text = element_blank(),
        panel.grid.major = element_blank(),
        strip.text = element_text(size = 13))
map_region_rmse
```
@fig-cv-noaa-regions shows that performance improves in all regions and leads to greater consistency in the RMSE across CONUS with slightly higher RMSE remaining in the Northwest and West regions.

```{r}
#| label: tbl-cv-month
#| tbl-cap: Performance by month of year.
result <- dt[,
    keyby = .(Month = month(lubridate::as_datetime(time.sat))),
    eval(performance.j)]
result[, Month := month.abb[Month]]
flextable(pretty.table.numbers(result))
```

```{r}
#| label: tbl-cv-high-aod
#| tbl-cap: Performance for higher versus lower AOD (AERONET \> 0.6).
by.highaod = dt[, eval(performance.j), by = .(aod_high = y.ground > 0.6)]
flextable(pretty.table.numbers(by.highaod))
```

```{r}
#| label: tbl-cv-qa-best
#| tbl-cap: Performance by quality flag `qa_best`.
by.qabest = dt[, eval(performance.j), by = qa_best]
flextable(pretty.table.numbers(by.qabest))
```

The takeaway here is that even though the RMSE of MAIAC is substantially better for `qa_best` versus non-best pixels, the improvements in the predicted AOD make even the non-best pixels have lower RMSE than the raw MAIAC `qa_best` and in that sense, we are expanding the set of high quality data by correcting for biases related to QA flagging criteria.

## Error Envelope

The most important difference in our comparison of MAIAC AOD with AERONET observations versus prior validation analyses is the single-pixel comparison whereas prior analyses use averaging over much larger space and time regions. In this manuscript, we only include comparisons over CONUS which limits the diversity of aerosol properties and most training data are in relatively low AOD conditions.

We use Nelder-Mead optimization to construct new error envelopes with coverage as close to 2/3 as possible for each of two subsets of the data: observations less than or equal to 0.6, and observations greater than 0.6. The results are an envelope with `r eee = \(pred) {o = empirical.error.envelope(obs = dt$y.ground, pred); stopifnot(o$convergence == 0); o$par}; show.eee = \(pred) {ps = eee(pred); sprintf("additive part %.03f and multiplicative part %.03f", ps[1], ps[2])}; show.eee(dt$y.sat)` for the raw satellite values, and an envelope with `r show.eee(dt$y.ground.pred)` for our predictions. The mean widths of the envelopes across all observations are `r me = \(pred) {ps = eee(pred); sprintf("%.03f", 2*ps[1] + 2*ps[2]*mean(dt$y.ground))}; me(dt$y.sat)` for the raw values and `r me(dt$y.ground.pred)` for our predictions. The original error envelope (reported in the AMT MAIAC publication), applied to the raw satellite values, has coverage `r x = envelope.tester(obs = dt$y.ground, pred = dt$y.sat)(.05, .1); sprintf("%.03f", x["lo"])` in the low subset and `r sprintf("%.03f", x["hi"])` in the high subset.

## Agreement Plots

```{r agreement_plot}
#| label: fig-agreement-plot
#| fig-cap: !expr caption
caption = with(list(d = agreement.plot.data(dt)), sprintf(
    "Agreement plots for satellite-based and AERONET AOD. Straight lines show empirical error envelopes containing 2/3 of retrievals. Not shown: %s %s and %s %s",
    d[!in.region & comparison == "y.sat", .N],
    "points outside left panel",
    d[!in.region & comparison == "y.ground.pred", .N],
    "points outside right panel"))
IG(tar_read(agreement.plot.path))
```

## SHAP interpretation

```{r shap_summary}
#| label: fig-shap-summary
#| fig-cap: Summary of SHAP contributions. Predictors ordered by decending mean absolute SHAP (printed along left).
IG(tar_read(shap.summary.plot.path))
```

```{r shap_features}
#| label: fig-shap-timeseries
#| fig-cap: Time series of SHAPs for Column Water Vapor (CWV). SHAPs for observations with missing values for CWV are shown in grey.
IG(tar_read(shap.features.plot.path))
```

## New predictions

For the model trained on all the data, the selected hyperparameters were `r v = tar_read(model.full)$hyperparams; paste0(collapse = ", ", sprintf("%s = %s", names(v), v))`.

### Mapping a typical day

@fig-map-median-mse shows a tile on `r tar_read(median.mse.map.data)$date`.

```{r pred.map.median.mse}
#| label: fig-map-median-mse
#| fig-cap: !expr caption
caption = sprintf("Maps of typical MAIAC and Corrected AOD. Day selected as having the median improvement in MSE at AERONET sites. The map is centered on %s.",
    tar_read(median.mse.map)$center.name)
IG(tar_read(pred.map.median.mse.path))
```

### Mapping a documented high AOD event

```{r pred.map.baltimore}
#| label: fig-map-baltimore-smoke
#| fig-cap: Maps of MAIAC and Corrected AOD over the Mid-Atlantic US during a documented exceedence event with downmixing of long-range transported smoke from Canadian wildfires.
IG(tar_read(pred.map.baltimore.path))
```

## Comparison with AQS PM~2.5~

We examined `r l = tar_read(satellite.vs.aqs); scales::comma(l$nobs)` observations of AQS PM~2.5~ on `r scales::comma(l$celldays)` cell-days. The correlation of the observations with the original satellite values was `r sprintf("%.03f", l$old)`, compared to `r sprintf("%.03f", l$new)` with our corrected values.

# Discussion

This project arose from a need to subset AOD datasets to exclude improbable values that were contaminating statistical relationships with ground-level observations of fine particulate matter (PM~2.5~) and a desire to derive data-driven rules for cleaning EO for Health and Air Quality applications.

We hypothesized that the time-series methods of MAIAC might benefit from having multiple overpasses in the same day and indeed saw lower bias and lower MAIAC RMSE for observations from site-days with multiple overpasses, that was still substantially improved by our correction. We did not investigate any potential geophysical basis for the regional variation that we saw in raw MAIAC AOD RMSE, but note that the substantially higher SD in the Northwest US region is likely driven by the contribution of episodic wildfire-related higher AOD conditions. Applying our correction decreased the differences in RMSE between regions, diminishing spatial variation in retrieval error. Similarly, although the `QA_best` quality control flag is strongly related to AOD RMSE, we found that applying the `Earth_obs_cleaning` workflow made the corrected low-quality AOD retrievals have a lower RMSE (better accuracy) than the RMSE for the MAIAC high-quality AOD retrievals.

An interesting consequence of empirical tuning with our workflow is that addressing sensor-specific sources of retrieval error can lead to more harmonious datasets between platforms, a key goal for NASA's long-term Earth System Data Records. For example, we previously showed that MAIAC CWV from the aging Terra MODIS had worse agreement with AERONET CWV than that of Aqua MODIS, particularly in more recent years, yet the remaining retrieval error after our bias-correction for Terra and Aqua CWV was virtually the same.[8](https://paperpile.com/c/ze3giJ/4wlV) Thus, empirical correction of aerosol observations from, for example, the VIIRS instruments that continues the legacy of MODIS,[13](https://paperpile.com/c/ze3giJ/zr7Y) would be expected to result in greater consistency of the aerosols Earth System Data Records. Our workflow uses ancillary and derived quality control features for a data-driven refinement of retrievals to make EO more reliable and useful to a broader community of end users.

The premise that satellite retrievals representing an area are comparable to point-based observations is common in product validation and among the most frequent uses of the often cited AERONET products. We use an unusually stringent space-time matchup criteria (single pixel with most observations within a few minutes) in order to better match up the air mass being observed. The reliance on the equivalence with ground-based observations might be a greater limitation for Earth observations with larger pixel sizes.

Our workflow treats all cloud-screened and quality-assured (Level 2.0) observations from AERONET instruments as reference data and does not require a minimum number of days of observation from a given site for inclusion (as might be desired in an analysis of long-term MODIS sensor drift). The restriction to this highest quality subset of AERONET means that the training set may have few or no examples of tricky retrieval conditions (e.g., cloud edges). Similarly, nearly all AERONET stations are over land and thus may not adequately represent retrieval errors over water and mixed pixels, however the MAIAC algorithm itself is applicable over land and inland water (and not the global ocean). We leave for a future analysis the applicability of a trained `Earth_obs_cleaning` model to other space-time regions (with similar or different aerosol properties). Although here we present the `Earth_obs_cleaning` trained on over 100,000 collocated observations, we use machine-learning methods likely to perform well with substantially smaller training datasets such as would be available for more recently launched sensors or over regions with more sparse ground network coverage.

Our workflow is not a gap-filling method and thus corrected results have the same coverage as the original EO product (although we rescue retrievals flagged as low quality), however such approaches which include assimilation and spatial interpolation could be applied after our correction method (which involves minimal borrowing of spatial information no more than 5 km from each grid cell).

# Data availability

# Supplement

@tbl-ivs shows the distribution of each predictor in the CV.

```{r tbl-ivs}
#| tbl-cap: !expr 'sprintf("`time.sat` is represented in days since %s.", as.character(min(dt$time.sat), usetz = T))'
flextable(rbindlist(lapply(Wf$features, \(iv)
   {nv = dt[[iv]]
    v = na.omit(nv[!is.na(nv)])
    r = \(x) round(x, 3)
    if (iv == "time.sat")
        v = as.numeric(difftime(v, min(v), units = "days"))
    data.table(
        Variable = iv,
        missing = r(mean(is.na(nv))),
        min = r(min(v)),
        max = r(max(v)),
        mean = r(mean(v)),
        sd = r(sd(v)))})))
```

```{r}
#| label: tbl-cv-by-year
#| tbl-cap: Cross-validation performance by year.
flextable(pretty.table.numbers(tar_read(cv.summary)[-1]))
```

```{r}
#| label: tbl-cv-overpasses
#| tbl-cap: Cross-validation performance by the number of overpasses in collocation for that site-day.
dt[, overpasses := .N, by = .(site, lubridate::as_date(time.ground))]
by.overpasses = dt[, eval(performance.j), keyby = overpasses]
flextable(pretty.table.numbers(by.overpasses))
```

# Author contributions

# Competing interests

The authors declare that they have no conflict of interest.

# Acknowledgments

We thank the AERONET federation principal investigators and their staff for establishing and maintaining the `r uniqueN(dt$site)` sun photometer sites used in this investigation.

# Financial support

This research has been supported by the National Institutes of Health Office of the Director (grant no. UH3 OD023337), the National Institutes of Health National Institute of Environmental Health Sciences (grant nos. P30 ES023515 and R01 ES031295).

# References
